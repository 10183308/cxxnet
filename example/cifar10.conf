# example configure file for mnist
# training iterator
data = train
iter = cifar
    path = "../data/cifar10/"
    input_flat = 0
    shuffle = 1
    batch1 = 1
    batch2 = 1
    batch3 = 1
    batch4 = 1
    batch5 = 1
    silent = 0
    image_mean = "cifar_mean.bin"
iter = end
# evaluation iterator
eval = test
iter = cifar
    path = "../data/cifar10/"
    input_flat = 0
    test = 1
    silent = 0
    image_mean = "cifar_mean.bin"
iter = end

netconfig=start
layer[0->1] = conv
  kernel_size = 5
  stride = 1
  pad = 2
  nchannel = 96
  random_type = 1
layer[1->2] = max_pooling
  kernel_size = 3
  stride = 2
layer[2->3] = relu
layer[3->4] = lrn
  local_size = 3
  alpha = 1.0
  beta = 1.0
  knorm = 1
####################
layer[4->5] = conv
  kernel_size = 5
  stride = 1
  pad = 2
  nchannel = 64
  random_type = 1
layer[5->6] = lrn
  local_size = 3
  alpha = 1.0
  beta = 1.0
  knorm = 1
layer[6->7] = max_pooling
  kernel_size = 3
  stride = 2
layer[7->8] = relu
layer[8->9] = conv
  kernel_size = 3
  stride = 1
  pad = 1
  nchannel = 64
  random_type = 1
layer[9->10] = flatten
layer[10->10] = dropout
  threshold = 0.3
layer[10->11] = fullc
  nhidden = 1024
  random_type = 1
layer[11->12] = relu
layer[12->13] = fullc
  nhidden = 10
  random_type = 1
layer[13->13] = softmax
netconfig=end

save_model = 0
# evaluation metric
metric = rec@1
max_round = 24
num_round = 24

# input shape not including batch
input_shape = 3,32,32
batch_size = 100

# global parameters in any sectiion outside netconfig, and iter
eta = 0.01
momentum = 0.9
wd  = 0.0005
dev=gpu
